import os
import streamlit as st
import uuid
from datetime import datetime
import json
from pathlib import Path

from langchain.document_loaders import TextLoader, CSVLoader
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader, UnstructuredExcelLoader
from langchain.memory import ConversationBufferMemory
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI

# from dotenv import load_dotenv
# load_dotenv()

# # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
# ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
# correct_username = os.getenv("LOGIN_USERNAME")
# correct_password = os.getenv("LOGIN_PASSWORD")
OPENAI_API_KEY = st.secrets("OPENAI_API_KEY")
GOOGLE_API_KEY = st.secrets("GOOGLE_API_KEY")
ANTHROPIC_API_KEY = st.secrets("ANTHROPIC_API_KEY")
correct_username = st.secrets("LOGIN_USERNAME")
correct_password = st.secrets("LOGIN_PASSWORD")


# å®šæ•°
CHAT_HISTORY_FILE = "chat_history.json"
TEMP_DIR = "temp_uploads"
VECTOR_STORE_DIR = "vector_store"

# APIã‚­ãƒ¼ã®ãƒã‚§ãƒƒã‚¯
def check_api_keys():
    missing_keys = []
    if not OPENAI_API_KEY:
        missing_keys.append("OPENAI_API_KEY")
    if not GOOGLE_API_KEY:
        missing_keys.append("GOOGLE_API_KEY")
    if not ANTHROPIC_API_KEY:
        missing_keys.append("ANTHROPIC_API_KEY")
    
    if missing_keys:
        st.error(f"ç’°å¢ƒå¤‰æ•°ã«æ¬¡ã®APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {', '.join(missing_keys)}")
        return False
    return True

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
for directory in [TEMP_DIR, VECTOR_STORE_DIR]:
    if not os.path.exists(directory):
        os.makedirs(directory)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
def init_session_state():
    if 'authenticated' not in st.session_state:
        st.session_state.authenticated = False
    if 'chat_id' not in st.session_state:
        st.session_state.chat_id = str(uuid.uuid4())
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = load_chat_history()
    if 'uploaded_files' not in st.session_state:
        st.session_state.uploaded_files = []
    if 'current_title' not in st.session_state:
        st.session_state.current_title = "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"
    if 'memory' not in st.session_state:
        st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®èª­ã¿è¾¼ã¿
def load_chat_history():
    try:
        if os.path.exists(CHAT_HISTORY_FILE):
            with open(CHAT_HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    except Exception as e:
        st.error(f"ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ä¿å­˜
def save_chat_history():
    try:
        with open(CHAT_HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(st.session_state.chat_history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

# æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã®ä½œæˆ
def create_new_chat():
    st.session_state.chat_id = str(uuid.uuid4())
    st.session_state.messages = []
    st.session_state.uploaded_files = []
    st.session_state.current_title = "æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# éå»ã®ãƒãƒ£ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€
def load_chat(chat_id):
    if chat_id in st.session_state.chat_history:
        st.session_state.chat_id = chat_id
        st.session_state.messages = st.session_state.chat_history[chat_id]["messages"]
        st.session_state.current_title = st.session_state.chat_history[chat_id]["title"]
        st.session_state.uploaded_files = st.session_state.chat_history[chat_id].get("uploaded_files", [])
        
        # ãƒ¡ãƒ¢ãƒªã®å†æ§‹ç¯‰
        memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
        for msg in st.session_state.messages:
            if msg["role"] == "user":
                memory.chat_memory.add_user_message(msg["content"])
            elif msg["role"] == "assistant":
                memory.chat_memory.add_ai_message(msg["content"])
        st.session_state.memory = memory

# ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†
def process_uploaded_file(uploaded_file):
    file_path = os.path.join(TEMP_DIR, uploaded_file.name)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã«åŸºã¥ã„ã¦ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’é¸æŠ
    file_extension = Path(uploaded_file.name).suffix.lower()
    documents = []
    
    try:
        if file_extension == ".txt":
            loader = TextLoader(file_path)
            documents = loader.load()
        elif file_extension == ".xlsx":
            loader = UnstructuredExcelLoader(file_path)
            documents = loader.load()
        else:
            st.error(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™: {file_extension}")
            return None
        
        # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        split_documents = text_splitter.split_documents(documents)
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=GOOGLE_API_KEY
        )
        vector_store = Chroma.from_documents(
            documents=split_documents,
            embedding=embeddings,
            persist_directory=os.path.join(VECTOR_STORE_DIR, st.session_state.chat_id)
        )
        
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«è¿½åŠ 
        file_info = {
            "name": uploaded_file.name,
            "path": file_path,
            "timestamp": datetime.now().isoformat()
        }
        st.session_state.uploaded_files.append(file_info)
        
        return vector_store
    
    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# LLMãƒ¢ãƒ‡ãƒ«ã®å–å¾—
def get_llm(model_name):
    if model_name == "gemini-2.0-flash-lite":
        return ChatGoogleGenerativeAI(model="gemini-2.0-flash-lite", temperature=0.7, google_api_key=GOOGLE_API_KEY)
    elif model_name == "gpt-4o-mini":
        return ChatOpenAI(model_name="gpt-4o-mini", temperature=0.7, openai_api_key=OPENAI_API_KEY)
    elif model_name == "claude-3-7-sonnet-latest":
        return ChatAnthropic(model_name="claude-3-7-sonnet-20240229", temperature=0.7, anthropic_api_key=ANTHROPIC_API_KEY)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«
        return ChatGoogleGenerativeAI(model="gemini-2.0-flash-lite", temperature=0.7, google_api_key=GOOGLE_API_KEY)

# ãƒãƒ£ãƒƒãƒˆã®å¿œç­”ã‚’ç”Ÿæˆ
def generate_response(user_input, model_name, uploaded_files):
    try:
        llm = get_llm(model_name)
        
        if uploaded_files:
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã€RAGã‚’ä½¿ç”¨
            vector_store = None
            for file_info in uploaded_files:
                file_path = file_info["path"]
                if os.path.exists(file_path):
                    file_extension = Path(file_path).suffix.lower()
                    documents = []
                    
                    if file_extension == ".txt":
                        loader = TextLoader(file_path)
                        documents = loader.load()
                    elif file_extension == ".xlsx":
                        loader = UnstructuredExcelLoader(file_path)
                        documents = loader.load()
                    
                    if documents:
                        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
                        split_documents = text_splitter.split_documents(documents)
                        
                        embeddings = GoogleGenerativeAIEmbeddings(
                            model="models/embedding-001",
                            google_api_key=GOOGLE_API_KEY
                        )
                        if vector_store is None:
                            vector_store = Chroma.from_documents(
                                documents=split_documents,
                                embedding=embeddings,
                                persist_directory=os.path.join(VECTOR_STORE_DIR, st.session_state.chat_id)
                            )
                        else:
                            vector_store.add_documents(split_documents)
            
            if vector_store:
                qa_chain = ConversationalRetrievalChain.from_llm(
                    llm=llm,
                    retriever=vector_store.as_retriever(),
                    memory=st.session_state.memory
                )
                response = qa_chain.invoke({"question": user_input})
                return response["answer"]
        
        # é€šå¸¸ã®ãƒãƒ£ãƒƒãƒˆå¿œç­”ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆï¼‰
        prompt = f"ã™ã¹ã¦ã®å›ç­”ã‚’æ—¥æœ¬èªã§è¿”ã—ã¦ãã ã•ã„ã€‚è³ªå•: {user_input}"
        response = llm.invoke(prompt)
        return response.content
    
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

# ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢
def login_page():
    st.title("ãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ã‚¤ãƒ³")
    
    with st.form("login_form"):
        username = st.text_input("ãƒ¦ãƒ¼ã‚¶ãƒ¼ID")
        password = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password")
        submit = st.form_submit_button("ãƒ­ã‚°ã‚¤ãƒ³")
        
        if submit:
            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã‚’å–å¾—
            correct_username = os.environ.get("LOGIN_USERNAME")
            correct_password = os.environ.get("LOGIN_PASSWORD")
            
            if username == correct_username and password == correct_password:
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸ")

# ãƒãƒ£ãƒƒãƒˆç”»é¢
def chat_page():
    st.title("LLMãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ")
    
    # ã‚µã‚¤ãƒ‰ãƒ¡ãƒ‹ãƒ¥ãƒ¼
    with st.sidebar:
        st.button("æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ", on_click=create_new_chat)
        
        st.subheader("éå»ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´")
        for chat_id, chat_data in st.session_state.chat_history.items():
            if st.button(chat_data["title"], key=f"chat_{chat_id}"):
                load_chat(chat_id)
                st.rerun()

        st.write("---")
        if st.button("ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ"):
            st.session_state.authenticated = False
            st.session_state.page = 1
            st.session_state.user_input = ""
            st.rerun()        
    
    # ãƒãƒ£ãƒƒãƒˆè¡¨ç¤º
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›é ˜åŸŸ
    col1, col2, col3 = st.columns([0.15, 0.15, 0.7])
    
    with col1:
        uploaded_file = st.file_uploader("ğŸ“", type=["txt", "xlsx"], label_visibility="collapsed")
        if uploaded_file:
            vector_store = process_uploaded_file(uploaded_file)
            if vector_store:
                st.success(f"{uploaded_file.name}ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ")
    
    with col2:
        model_options = ["gemini-2.0-flash-lite", "gpt-4o-mini", "claude-3-7-sonnet-latest"]
        selected_model = st.selectbox("ğŸ¤–", model_options, label_visibility="collapsed")
    
    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¿½åŠ 
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # å¿œç­”ã®ç”Ÿæˆ
        with st.chat_message("assistant"):
            with st.spinner("è€ƒãˆä¸­..."):
                response = generate_response(prompt, selected_model, st.session_state.uploaded_files)
                st.markdown(response)
        
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¿½åŠ 
        st.session_state.messages.append({"role": "assistant", "content": response})
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®æ›´æ–°
        if st.session_state.chat_id not in st.session_state.chat_history:
            # æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦ä½¿ç”¨
            title = prompt[:30] + "..." if len(prompt) > 30 else prompt
            st.session_state.current_title = title
            st.session_state.chat_history[st.session_state.chat_id] = {
                "title": title,
                "created_at": datetime.now().isoformat(),
                "messages": st.session_state.messages,
                "uploaded_files": st.session_state.uploaded_files
            }
        else:
            st.session_state.chat_history[st.session_state.chat_id]["messages"] = st.session_state.messages
            st.session_state.chat_history[st.session_state.chat_id]["uploaded_files"] = st.session_state.uploaded_files
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜
        save_chat_history()

# ãƒ¡ã‚¤ãƒ³é–¢æ•°
def main():
    st.set_page_config(page_title="ãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ", layout="wide")
    init_session_state()
    
    # APIã‚­ãƒ¼ã®ãƒã‚§ãƒƒã‚¯
    if not check_api_keys() and st.session_state.authenticated:
        st.warning("ä¸€éƒ¨ã®APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ä¸€éƒ¨ã®æ©Ÿèƒ½ãŒå‹•ä½œã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
    
    if not st.session_state.authenticated:
        login_page()
    else:
        chat_page()

if __name__ == "__main__":
    main()